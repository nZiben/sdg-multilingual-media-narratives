{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SDG Multilingual Media Narratives — 01: Collect News via GDELT (MAX 2024–2025)\n",
        "\n",
        "This notebook fetches **as much data as possible** from the GDELT DOC 2.0 **ArtList** endpoint for the period **2024-01-01 → 2025-12-31**.\n",
        "\n",
        "Key features:\n",
        "- Splits the full range into **≤ 90-day windows** (ArtList effectively focuses on recent ~3 months inside any range).\n",
        "- Paginates via **startrecord** to collect all pages in each window.\n",
        "- Robust request handling: retries, backoff, and readable error messages for non-JSON responses.\n",
        "- Saves **two datasets**:\n",
        "  - **RAW**: all fetched rows (includes duplicates across pagination/windows)\n",
        "  - **DEDUP**: unique articles by URL (recommended for analysis)\n",
        "\n",
        "Outputs (in `data/raw/`):\n",
        "- `gdelt_articles_raw_2024_2025.csv` (+ optional parquet)\n",
        "- `gdelt_articles_dedup_2024_2025.csv` (+ optional parquet)\n",
        "- `01_gdelt_collection_report_2024_2025.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import hashlib\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAW_DIR: /Users/sergey/code/sdg-multilingual-media-narratives/data/raw\n",
            "REPORTS_DIR: /Users/sergey/code/sdg-multilingual-media-narratives/reports\n"
          ]
        }
      ],
      "source": [
        "# ---------- Paths ----------\n",
        "PROJECT_DIR = os.path.abspath(\".\")\n",
        "RAW_DIR = os.path.join(PROJECT_DIR, \"data\", \"raw\")\n",
        "REPORTS_DIR = os.path.join(PROJECT_DIR, \"reports\")\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"RAW_DIR:\", RAW_DIR)\n",
        "print(\"REPORTS_DIR:\", REPORTS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query\n",
        "Tips:\n",
        "- GDELT treats anything inside **double quotes** as an *exact phrase*. Avoid quoting very short tokens like `\"SDG\"`.\n",
        "- If you use `OR`, wrap the whole expression in parentheses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUERY = (\"sustainable development\" OR \"sustainable development goals\" OR \"climate change\" OR \"global warming\" OR \"renewable energy\" OR \"public health\" OR poverty OR inequality OR biodiversity OR SDG OR SDGs)\n"
          ]
        }
      ],
      "source": [
        "SDG_TERMS = [\n",
        "    '\"sustainable development\"',\n",
        "    '\"sustainable development goals\"',\n",
        "    '\"climate change\"',\n",
        "    '\"global warming\"',\n",
        "    '\"renewable energy\"',\n",
        "    '\"public health\"',\n",
        "    \"poverty\",\n",
        "    \"inequality\",\n",
        "    \"biodiversity\",\n",
        "    \"SDG\",      \n",
        "    \"SDGs\",    \n",
        "]\n",
        "\n",
        "QUERY = \"(\" + \" OR \".join(SDG_TERMS) + \")\"\n",
        "print(\"QUERY =\", QUERY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Robust fetch helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "GDELT_DOC_ENDPOINT = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "\n",
        "def sha1_text(s: str) -> str:\n",
        "    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "\n",
        "def now_utc_iso() -> str:\n",
        "    return datetime.now(timezone.utc).replace(microsecond=0).isoformat()\n",
        "\n",
        "def parse_gdelt_date(date_str: str):\n",
        "    # GDELT often returns YYYYMMDDHHMMSS\n",
        "    if not isinstance(date_str, str):\n",
        "        return pd.NaT\n",
        "    if re.fullmatch(r\"\\d{14}\", date_str):\n",
        "        return pd.to_datetime(date_str, format=\"%Y%m%d%H%M%S\", errors=\"coerce\", utc=True)\n",
        "    return pd.to_datetime(date_str, errors=\"coerce\", utc=True)\n",
        "\n",
        "def normalize_whitespace(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.replace(\"\\u00a0\", \" \")\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def _balanced_parens(s: str) -> bool:\n",
        "    depth = 0\n",
        "    for ch in s:\n",
        "        if ch == \"(\":\n",
        "            depth += 1\n",
        "        elif ch == \")\":\n",
        "            depth -= 1\n",
        "            if depth < 0:\n",
        "                return False\n",
        "    return depth == 0\n",
        "\n",
        "def normalize_query_for_gdelt(q: str) -> str:\n",
        "    # OR blocks must be wrapped in parentheses.\n",
        "    q = (q or \"\").strip()\n",
        "    if \" OR \" in q and not (q.startswith(\"(\") and q.endswith(\")\") and _balanced_parens(q)):\n",
        "        q = f\"({q})\"\n",
        "    return q\n",
        "\n",
        "def _gdelt_get_json(params: dict, max_retries: int = 6, timeout: int = 45):\n",
        "    \"\"\"Robust JSON fetch for GDELT.\n",
        "\n",
        "    Handles cases where the API returns HTML/text (invalid params, short phrases, throttling, etc).\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (compatible; sdg-research-bot/0.1; +https://gdeltproject.org)\"\n",
        "    }\n",
        "\n",
        "    last_err = None\n",
        "    for attempt in range(max_retries):\n",
        "        r = requests.get(GDELT_DOC_ENDPOINT, params=params, headers=headers, timeout=timeout)\n",
        "\n",
        "        # Retry on transient failures / throttling\n",
        "        if r.status_code in (429, 500, 502, 503, 504):\n",
        "            time.sleep(min(30, 1.5 ** attempt) + random.random())\n",
        "            continue\n",
        "\n",
        "        r.raise_for_status()\n",
        "\n",
        "        text = (r.text or \"\").strip()\n",
        "        ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
        "\n",
        "        if not text:\n",
        "            last_err = RuntimeError(\"Empty response body from GDELT (status 200).\")\n",
        "        elif text[0] not in \"{[\":\n",
        "            last_err = RuntimeError(\n",
        "                \"Non-JSON response from GDELT. \"\n",
        "                f\"Content-Type={ctype!r}. First 300 chars:\\n{text[:300]}\"\n",
        "            )\n",
        "        else:\n",
        "            try:\n",
        "                return r.json()\n",
        "            except Exception as e:\n",
        "                last_err = RuntimeError(\n",
        "                    \"Failed to parse JSON. \"\n",
        "                    f\"Content-Type={ctype!r}. First 300 chars:\\n{text[:300]}\\nError={e}\"\n",
        "                )\n",
        "\n",
        "        time.sleep(min(20, 1.5 ** attempt) + random.random())\n",
        "\n",
        "    raise last_err or RuntimeError(\"GDELT request failed after retries.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Max collection for 2024–2025 (windowed)\n",
        "\n",
        "Why windows?\n",
        "- With DOC 2.0 **ArtList**, very long `startdatetime → enddatetime` ranges do not reliably return deep history.\n",
        "- The common workaround is to split into **≤ 90-day windows** and iterate.\n",
        "\n",
        "Tune:\n",
        "- `WINDOW_DAYS`: 30–90 (90 is usually efficient)\n",
        "- `PAGE_SIZE`: 200–250 (try 250 first)\n",
        "- `SLEEP_BETWEEN_CALLS`: be polite; increase if you see 429s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Collection parameters ----------\n",
        "WINDOW_DAYS = 90\n",
        "PAGE_SIZE = 250\n",
        "SLEEP_BETWEEN_CALLS = 0.35\n",
        "\n",
        "START = datetime(2024, 1, 1, tzinfo=timezone.utc)\n",
        "END = datetime(2025, 12, 31, 23, 59, 59, tzinfo=timezone.utc)\n",
        "\n",
        "def fmt_gdelt_dt(dt: datetime) -> str:\n",
        "    return dt.astimezone(timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "def iter_windows(start: datetime, end: datetime, window_days: int = WINDOW_DAYS):\n",
        "    cur = start\n",
        "    delta = timedelta(days=window_days)\n",
        "    while cur < end:\n",
        "        nxt = min(end, cur + delta)\n",
        "        yield cur, nxt\n",
        "        cur = nxt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_artlist_window(query: str, start_dt: str, end_dt: str, page_size: int = PAGE_SIZE):\n",
        "    \"\"\"\n",
        "    MAX collector for a time window using *adaptive time splitting*.\n",
        "\n",
        "    Why: DOC 2.0 ArtList returns up to 250 results (maxrecords) per request.\n",
        "    If a window returns exactly page_size, it likely hit the cap, so we split the window\n",
        "    and fetch both halves to get more results.\n",
        "\n",
        "    Inputs:\n",
        "      - start_dt/end_dt: YYYYMMDDHHMMSS (UTC)\n",
        "    Returns:\n",
        "      - list[dict] of raw GDELT article records for that window (can be > page_size due to splitting)\n",
        "    \"\"\"\n",
        "    # Hard cap per docs: maxrecords up to 250\n",
        "    page_size = min(int(page_size), 250)\n",
        "\n",
        "    def _str_to_dt(s: str) -> datetime:\n",
        "        return datetime.strptime(s, \"%Y%m%d%H%M%S\").replace(tzinfo=timezone.utc)\n",
        "\n",
        "    def _dt_to_str(dt: datetime) -> str:\n",
        "        return dt.astimezone(timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "    # Tune these if needed\n",
        "    MIN_WINDOW = timedelta(hours=9)     # smallest window to split down to (reduce for more data, increase for fewer calls)\n",
        "    MAX_DEPTH = 20                     # safety guard\n",
        "    OVERLAP_GUARD = timedelta(seconds=1)\n",
        "\n",
        "    def _fetch_once(sdt: str, edt: str) -> list:\n",
        "        params = {\n",
        "            \"query\": query,\n",
        "            \"mode\": \"ArtList\",\n",
        "            \"format\": \"json\",\n",
        "            \"maxrecords\": page_size,\n",
        "            \"startdatetime\": sdt,\n",
        "            \"enddatetime\": edt,\n",
        "            \"sort\": \"DateDesc\",\n",
        "        }\n",
        "        try:\n",
        "            payload = _gdelt_get_json(params)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ request failed, skipping shard/page: {repr(e)}\")\n",
        "            return []\n",
        "        arts = payload.get(\"articles\", []) or []\n",
        "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
        "        return arts\n",
        "\n",
        "    def _split_fetch(s: datetime, e: datetime, depth: int = 0) -> list:\n",
        "        sdt, edt = _dt_to_str(s), _dt_to_str(e)\n",
        "        arts = _fetch_once(sdt, edt)\n",
        "\n",
        "        # Not capped -> done\n",
        "        if len(arts) < page_size:\n",
        "            return arts\n",
        "\n",
        "        # Capped -> try split if window still large enough\n",
        "        if depth >= MAX_DEPTH or (e - s) <= MIN_WINDOW:\n",
        "            # We cannot split further; return capped results (best we can do at this granularity)\n",
        "            print(f\"⚠️ Window hit cap={page_size} and cannot split further: {sdt} -> {edt} (depth={depth})\")\n",
        "            return arts\n",
        "\n",
        "        # Split at midpoint; avoid double-counting the boundary by shifting right window start by 1s\n",
        "        mid = s + (e - s) / 2\n",
        "        mid = mid.replace(microsecond=0)\n",
        "        right_start = mid + OVERLAP_GUARD\n",
        "        if right_start >= e:\n",
        "            # Degenerate split; stop splitting\n",
        "            print(f\"⚠️ Degenerate split prevented: {sdt} -> {edt} (depth={depth})\")\n",
        "            return arts\n",
        "\n",
        "        left = _split_fetch(s, mid, depth + 1)\n",
        "        right = _split_fetch(right_start, e, depth + 1)\n",
        "\n",
        "        # Deduplicate within this merged window by URL (GDELT can return repeats across close boundaries)\n",
        "        seen = set()\n",
        "        merged = []\n",
        "        for r in left + right:\n",
        "            if not isinstance(r, dict):\n",
        "                continue\n",
        "            u = r.get(\"url\")\n",
        "            if not u or u in seen:\n",
        "                continue\n",
        "            seen.add(u)\n",
        "            merged.append(r)\n",
        "\n",
        "        return merged\n",
        "\n",
        "    s0 = _str_to_dt(start_dt)\n",
        "    e0 = _str_to_dt(end_dt)\n",
        "    print(\"progress:\", start_dt, \"->\", end_dt)\n",
        "    return _split_fetch(s0, e0, depth=0)\n",
        "\n",
        "\n",
        "def normalize_gdelt_record(r: dict) -> dict:\n",
        "    url = r.get(\"url\")\n",
        "    title = normalize_whitespace(r.get(\"title\", \"\"))\n",
        "    snippet = normalize_whitespace(r.get(\"snippet\", \"\"))\n",
        "    seendate = parse_gdelt_date(r.get(\"seendate\", \"\"))\n",
        "    source_country = r.get(\"sourceCountry\")\n",
        "    source_lang = r.get(\"language\") or r.get(\"sourceLanguage\")\n",
        "    domain = r.get(\"domain\")\n",
        "\n",
        "    uid = sha1_text((url or \"\") + \"|\" + title + \"|\" + snippet)\n",
        "\n",
        "    return {\n",
        "        \"id\": uid,\n",
        "        \"url\": url,\n",
        "        \"title\": title,\n",
        "        \"snippet\": snippet,\n",
        "        \"seendate_utc\": seendate,\n",
        "        \"domain\": domain,\n",
        "        \"source_country\": source_country,\n",
        "        \"language\": source_lang,\n",
        "        \"gdelt_raw\": json.dumps(r, ensure_ascii=False),\n",
        "        \"collected_at_utc\": now_utc_iso(),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_gdelt_max_2024_2025(query: str):\n",
        "    query = normalize_query_for_gdelt(query)\n",
        "\n",
        "    raw_rows = []\n",
        "    dedup_rows = []\n",
        "    seen_urls = set()\n",
        "    failures = []  # keep errors but do not stop\n",
        "\n",
        "    for w_start, w_end in iter_windows(START, END, WINDOW_DAYS):\n",
        "        sdt = fmt_gdelt_dt(w_start)\n",
        "        edt = fmt_gdelt_dt(w_end)\n",
        "\n",
        "        print(f\"Window {sdt} -> {edt}\")\n",
        "\n",
        "        try:\n",
        "            raw = fetch_artlist_window(query, sdt, edt, page_size=PAGE_SIZE)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Window failed (skipping): {sdt} -> {edt}\")\n",
        "            \n",
        "            print(\"   Error:\", repr(e))\n",
        "            failures.append({\"start\": sdt, \"end\": edt, \"error\": repr(e)})\n",
        "\n",
        "            # checkpoint what we already have\n",
        "            if raw_rows:\n",
        "                pd.DataFrame(raw_rows).to_csv(\n",
        "                    os.path.join(RAW_DIR, \"gdelt_articles_raw_2024_2025_checkpoint.csv\"),\n",
        "                    index=False\n",
        "                )\n",
        "            if dedup_rows:\n",
        "                pd.DataFrame(dedup_rows).to_csv(\n",
        "                    os.path.join(RAW_DIR, \"gdelt_articles_dedup_2024_2025_checkpoint.csv\"),\n",
        "                    index=False\n",
        "                )\n",
        "            continue\n",
        "\n",
        "        print(\"  fetched:\", len(raw))\n",
        "\n",
        "        norm = [normalize_gdelt_record(r) for r in raw if isinstance(r, dict)]\n",
        "        raw_rows.extend(norm)\n",
        "\n",
        "        added = 0\n",
        "        for row in norm:\n",
        "            u = row.get(\"url\")\n",
        "            if not u:\n",
        "                continue\n",
        "            if u in seen_urls:\n",
        "                continue\n",
        "            seen_urls.add(u)\n",
        "            dedup_rows.append(row)\n",
        "            added += 1\n",
        "\n",
        "        print(\"  unique urls added:\", added, \"| total unique:\", len(seen_urls))\n",
        "        print(\"  cumulative raw:\", len(raw_rows))\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # checkpoint after every window\n",
        "        pd.DataFrame(raw_rows).to_csv(os.path.join(RAW_DIR, \"gdelt_articles_raw_2024_2025_checkpoint.csv\"), index=False)\n",
        "        pd.DataFrame(dedup_rows).to_csv(os.path.join(RAW_DIR, \"gdelt_articles_dedup_2024_2025_checkpoint.csv\"), index=False)\n",
        "\n",
        "    # save failures log\n",
        "    if failures:\n",
        "        fail_path = os.path.join(REPORTS_DIR, \"01_gdelt_failures_2024_2025.json\")\n",
        "        with open(fail_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(failures, f, ensure_ascii=False, indent=2)\n",
        "        print(\"Saved failures log:\", fail_path)\n",
        "\n",
        "    df_raw = pd.DataFrame(raw_rows)\n",
        "    df_dedup = pd.DataFrame(dedup_rows)\n",
        "    return df_raw, df_dedup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run collection (this may take a while depending on volume and rate limits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_raw, df_dedup = fetch_gdelt_max_2024_2025(QUERY)\n",
        "\n",
        "print(\"DONE\")\n",
        "print(\"raw shape   :\", df_raw.shape)\n",
        "print(\"dedup shape :\", df_dedup.shape)\n",
        "\n",
        "# sanity: duplication ratio\n",
        "raw_urls = df_raw[\"url\"].nunique(dropna=True) if len(df_raw) else 0\n",
        "dedup_urls = df_dedup[\"url\"].nunique(dropna=True) if len(df_dedup) else 0\n",
        "print(\"raw unique urls  :\", raw_urls)\n",
        "print(\"dedup unique urls:\", dedup_urls)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c88ce51c",
      "metadata": {},
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6593922d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded from checkpoints:\n",
            " raw  : (56000, 10)\n",
            " dedup: (56000, 10)\n"
          ]
        }
      ],
      "source": [
        "import os, pandas as pd\n",
        "\n",
        "raw_ckpt = os.path.join(RAW_DIR, \"gdelt_articles_raw_2024_2025_checkpoint.csv\")\n",
        "ded_ckpt = os.path.join(RAW_DIR, \"gdelt_articles_dedup_2024_2025_checkpoint.csv\")\n",
        "\n",
        "assert os.path.exists(raw_ckpt), f\"Checkpoint not found: {raw_ckpt}\"\n",
        "assert os.path.exists(ded_ckpt), f\"Checkpoint not found: {ded_ckpt}\"\n",
        "\n",
        "df_raw = pd.read_csv(raw_ckpt)\n",
        "df_dedup = pd.read_csv(ded_ckpt)\n",
        "\n",
        "print(\"Loaded from checkpoints:\")\n",
        "print(\" raw  :\", df_raw.shape)\n",
        "print(\" dedup:\", df_dedup.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect duplicates (why raw > dedup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top duplicated URLs in RAW:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "url\n",
              "https://www.newsit.gr/oikonomia/elliniki-oikonomia-2024-oi-anoixtes-prokliseis-kai-to-astathes-diethnes-perivallon-polemon-kai-eklogon-se-ee-kai-ipa/3942401/                                                                                1\n",
              "https://www.lowellsun.com/2024/01/01/beacon-hill-roll-call-senate-support-of-gov-maura-healey/                                                                                                                                               1\n",
              "https://www.chelmsfordweeklynews.co.uk/news/national/24020417.energy-bill-price-hike-takes-effect-record-numbers-struggle-debt/                                                                                                              1\n",
              "https://www.protothema.gr/world/article/1451302/kina-etoimos-na-sunergastei-me-ton-baiden-gia-eirini-kai-anaptuxi-ston-kosmo-dilonei-o-si-tzinping/                                                                                          1\n",
              "https://www.hallandsposten.se/nyheter/v%C3%A4rlden/2024-%C3%A4r-ett-superval%C3%A5r-miljarder-v%C3%A4ntas-r%C3%B6sta-1.118722655                                                                                                             1\n",
              "https://www.jutarnji.hr/vijesti/svijet/ozbiljna-kriza-trese-britaniju-stotine-tisuca-ljudi-ne-moze-placati-racune-hranu-i-dugove-raste-broj-beskucnika-15410750                                                                              1\n",
              "https://imparcialoaxaca.mx/oaxaca/830276/requieren-300-mdp-para-proyecto-de-agua-en-los-valles/                                                                                                                                              1\n",
              "https://economy.okezone.com/read/2024/01/01/320/2947833/program-ganjar-mahfud-tegaskan-ekonomi-tumbuh-lapangan-kerja-tercipta-dan-lingkungan-terjaga                                                                                         1\n",
              "https://sofokleous10.gr/2024/01/01/%CF%84%CE%BF-%CE%B1%CF%8C%CF%81%CE%B1%CF%84%CE%BF-%CE%B5%CE%BC%CF%80%CF%8C%CF%81%CE%B9%CE%BF-%CF%84%CE%BF%CF%85-%CE%B5%CE%B9%CE%BA%CE%BF%CE%BD%CE%B9%CE%BA%CE%BF%CF%8D-%CE%BD%CE%B5%CF%81%CE%BF%CF%8D/    1\n",
              "https://www.faz.net/aktuell/wissen/erde-klima/klimawandel-im-himalaja-was-passiert-mit-den-gletschern-19378650.html                                                                                                                          1\n",
              "https://www.irishexaminer.com/lifestyle/outdoors/arid-41293522.html                                                                                                                                                                          1\n",
              "https://news.yam.md/ro/story/14825507                                                                                                                                                                                                        1\n",
              "https://www.punjabkesari.in/business/news/indian-government-aims-to-store-2-billion-tonnes-of-coal-by-2030-1922935                                                                                                                           1\n",
              "http://thepeninsulaqatar.com/article/01/01/2024/public-private-hospitals-to-link-records-of-patients                                                                                                                                         1\n",
              "https://www.johnogroat-journal.co.uk/news/national/energy-bill-price-hike-takes-effect-as-record-numbers-struggle-with-debt-96634/                                                                                                           1\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if len(df_raw):\n",
        "    vc = df_raw[\"url\"].value_counts().head(15)\n",
        "    print(\"Top duplicated URLs in RAW:\")\n",
        "    display(vc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save outputs (CSV + Parquet via pyarrow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved CSV:\n",
            " - /Users/sergey/code/sdg-multilingual-media-narratives/data/raw/gdelt_articles_raw_2024_2025.csv\n",
            " - /Users/sergey/code/sdg-multilingual-media-narratives/data/raw/gdelt_articles_dedup_2024_2025.csv\n"
          ]
        }
      ],
      "source": [
        "raw_csv = os.path.join(RAW_DIR, \"gdelt_articles_raw_2024_2025.csv\")\n",
        "dedup_csv = os.path.join(RAW_DIR, \"gdelt_articles_dedup_2024_2025.csv\")\n",
        "df_raw.to_csv(raw_csv, index=False)\n",
        "df_dedup.to_csv(dedup_csv, index=False)\n",
        "\n",
        "print(\"Saved CSV:\")\n",
        "print(\" -\", raw_csv)\n",
        "print(\" -\", dedup_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved Parquet (pyarrow):\n",
            " - /Users/sergey/code/sdg-multilingual-media-narratives/data/raw/gdelt_articles_raw_2024_2025.parquet\n",
            " - /Users/sergey/code/sdg-multilingual-media-narratives/data/raw/gdelt_articles_dedup_2024_2025.parquet\n"
          ]
        }
      ],
      "source": [
        "# Parquet saving via pyarrow (avoids pandas<->pyarrow extension-type issues)\n",
        "try:\n",
        "    import pyarrow as pa\n",
        "    import pyarrow.parquet as pq\n",
        "\n",
        "    raw_parquet = os.path.join(RAW_DIR, \"gdelt_articles_raw_2024_2025.parquet\")\n",
        "    dedup_parquet = os.path.join(RAW_DIR, \"gdelt_articles_dedup_2024_2025.parquet\")\n",
        "\n",
        "    tbl_raw = pa.Table.from_pandas(df_raw, preserve_index=False)\n",
        "    tbl_dedup = pa.Table.from_pandas(df_dedup, preserve_index=False)\n",
        "\n",
        "    pq.write_table(tbl_raw, raw_parquet, compression=\"snappy\")\n",
        "    pq.write_table(tbl_dedup, dedup_parquet, compression=\"snappy\")\n",
        "\n",
        "    print(\"Saved Parquet (pyarrow):\")\n",
        "    print(\" -\", raw_parquet)\n",
        "    print(\" -\", dedup_parquet)\n",
        "except Exception as e:\n",
        "    print(\"Parquet save skipped/failed:\", repr(e))\n",
        "    print(\"CSV files are saved and are sufficient for the next notebooks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write a small report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/Users/sergey/code/sdg-multilingual-media-narratives/reports/01_gdelt_collection_report_2024_2025.json',\n",
              " {'query': '(\"sustainable development\" OR \"sustainable development goals\" OR \"climate change\" OR \"global warming\" OR \"renewable energy\" OR \"public health\" OR poverty OR inequality OR biodiversity OR SDG OR SDGs)',\n",
              "  'start_utc': '2024-01-01T00:00:00+00:00',\n",
              "  'end_utc': '2025-12-31T23:59:59+00:00',\n",
              "  'window_days': 90,\n",
              "  'page_size': 250,\n",
              "  'sleep_between_calls': 0.35,\n",
              "  'raw_rows': 495520,\n",
              "  'dedup_rows': 495520,\n",
              "  'raw_unique_urls': 495520,\n",
              "  'dedup_unique_urls': 495520,\n",
              "  'generated_at_utc': '2026-01-31T19:29:03+00:00'})"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "report = {\n",
        "    \"query\": QUERY,\n",
        "    \"start_utc\": START.isoformat(),\n",
        "    \"end_utc\": END.isoformat(),\n",
        "    \"window_days\": WINDOW_DAYS,\n",
        "    \"page_size\": PAGE_SIZE,\n",
        "    \"sleep_between_calls\": SLEEP_BETWEEN_CALLS,\n",
        "    \"raw_rows\": int(df_raw.shape[0]),\n",
        "    \"dedup_rows\": int(df_dedup.shape[0]),\n",
        "    \"raw_unique_urls\": int(df_raw[\"url\"].nunique(dropna=True)) if len(df_raw) else 0,\n",
        "    \"dedup_unique_urls\": int(df_dedup[\"url\"].nunique(dropna=True)) if len(df_dedup) else 0,\n",
        "    \"generated_at_utc\": now_utc_iso(),\n",
        "}\n",
        "\n",
        "report_path = os.path.join(REPORTS_DIR, \"01_gdelt_collection_report_2024_2025.json\")\n",
        "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "report_path, report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6dd863f",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deb7a73f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (jupyter-venv)",
      "language": "python",
      "name": "jupyter-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
